{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "571b0449-b5b7-4a9c-89cb-e0f8e12a61d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tnn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as f\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b4608e2-139f-4dfc-8936-4f69c7b075f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ylecun/mnist\", num_proc=2)\n",
    "train_size = 10000\n",
    "test_size = 1000\n",
    "\n",
    "train = dataset.get(\"train\")\n",
    "test = dataset.get(\"test\")\n",
    "\n",
    "train_indices = np.random.choice(len(train), size=train_size, replace=False)\n",
    "test_indices = np.random.choice(len(test), size=test_size, replace=False)\n",
    "\n",
    "train = train.select(train_indices)\n",
    "test = test.select(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b48c375d-702b-4972-86c3-77e2c194dccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d68a4e57934538992e1119f4eb33fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ab80f046454f059e90c0e6ca8b8ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def to_numpy(example):\n",
    "    arr = np.reshape(example[\"image\"], -1) / 255.0\n",
    "    example[\"input\"] = arr\n",
    "    return example\n",
    "\n",
    "\n",
    "train_dataset = train.map(to_numpy, num_proc=2).select_columns([\"input\", \"label\"])\n",
    "test_dataset = test.map(to_numpy, num_proc=2).select_columns([\"input\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48769364-b4d3-421c-996a-cd55523d85a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    inputs = torch.tensor([ex[\"input\"] for ex in batch]).float()\n",
    "    labels = torch.tensor([ex[\"label\"] for ex in batch]).long()\n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "trainloader = data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=len(train_dataset),\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    ")\n",
    "testloader = data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=len(test_dataset),\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a160b8da-2a9c-4a34-ab79-38e19e8af72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(784, 512)\n",
    "        self.norm_1 = nn.LayerNorm(512)\n",
    "        self.drop_1 = nn.Dropout(0.4)\n",
    "        self.linear_2 = nn.Linear(512, 512)\n",
    "        self.norm_2 = nn.LayerNorm(512)\n",
    "        self.drop_2 = nn.Dropout(0.2)\n",
    "        self.linear_3 = nn.Linear(512, 512)\n",
    "        self.norm_3 = nn.LayerNorm(512)\n",
    "        self.drop_3 = nn.Dropout(0.2)\n",
    "        self.linear_4 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm_1(self.linear_1(x))\n",
    "        x = self.drop_1(f.relu(x))\n",
    "\n",
    "        x = self.norm_2(self.linear_2(x))\n",
    "        x = self.drop_2(f.relu(x))\n",
    "\n",
    "        x = self.norm_3(self.linear_3(x))\n",
    "        x = self.drop_3(f.relu(x))\n",
    "\n",
    "        x = f.relu(self.linear_4(x))\n",
    "        return {\"logits\": x}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b173cf89-c448-41a2-9ab1-f00385938714",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "411e3fb2-cc90-443a-8bf0-cbaa8e76dfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-2\n",
    "model = tnn.Model(MLP())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34bbe022-f35b-4a62-ae98-ff29cdf9cd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = tnn.Trainer(\n",
    "    model,\n",
    "    optim,\n",
    "    loss_fn,\n",
    "    trainloader,\n",
    "    testloader,\n",
    "    path=\"./batch.h5\",\n",
    "    device=device,\n",
    "    verbose=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b78b981-c0bc-41c2-a3ae-fc9891613bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model using cuda\n",
      "weights saved to ./batch.h5/trajectory/weights-epoch-0\n",
      "training started\n",
      "(epoch: 10): (train loss: 1.8252, test loss: 1.4482, train acc: 0.4764, test acc: 0.5930)\n",
      "weights saved to ./batch.h5/trajectory/weights-epoch-10\n",
      "(epoch: 20): (train loss: 1.0882, test loss: 0.7085, train acc: 0.7310, test acc: 0.8070)\n",
      "weights saved to ./batch.h5/trajectory/weights-epoch-20\n",
      "(epoch: 30): (train loss: 0.7735, test loss: 0.5069, train acc: 0.8008, test acc: 0.8480)\n",
      "weights saved to ./batch.h5/trajectory/weights-epoch-30\n",
      "(epoch: 40): (train loss: 0.6233, test loss: 0.4158, train acc: 0.8319, test acc: 0.8770)\n",
      "weights saved to ./batch.h5/trajectory/weights-epoch-40\n",
      "(epoch: 50): (train loss: 0.5426, test loss: 0.3629, train acc: 0.8501, test acc: 0.8960)\n",
      "weights saved to ./batch.h5/trajectory/weights-epoch-50\n",
      "(epoch: 60): (train loss: 0.4766, test loss: 0.3313, train acc: 0.8685, test acc: 0.9000)\n",
      "weights saved to ./batch.h5/trajectory/weights-epoch-60\n",
      "(epoch: 70): (train loss: 0.4308, test loss: 0.3091, train acc: 0.8791, test acc: 0.9090)\n",
      "weights saved to ./batch.h5/trajectory/weights-epoch-70\n",
      "(epoch: 80): (train loss: 0.3988, test loss: 0.2887, train acc: 0.8862, test acc: 0.9140)\n",
      "weights saved to ./batch.h5/trajectory/weights-epoch-80\n",
      "(epoch: 90): (train loss: 0.3773, test loss: 0.2734, train acc: 0.8891, test acc: 0.9160)\n",
      "weights saved to ./batch.h5/trajectory/weights-epoch-90\n",
      "(epoch: 100): (train loss: 0.3513, test loss: 0.2649, train acc: 0.8969, test acc: 0.9160)\n",
      "weights saved to ./batch.h5/trajectory/weights-epoch-100\n",
      "training complete\n",
      "train_losses saved to ./batch.h5/metrics/train_losses\n",
      "test_losses saved to ./batch.h5/metrics/test_losses\n",
      "train_accs saved to ./batch.h5/metrics/train_accs\n",
      "test_accs saved to ./batch.h5/metrics/test_accs\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.train(epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b4a9e4-11b5-4e76-aab7-028f73a8db59",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49995b56-5caf-4a6c-8955-05793915bd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4297b6b4-08df-47ed-a0dd-fef070b6cb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tnn.Model(MLP())\n",
    "optim = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a9c8eef-1331-4b68-b82b-775f43b9fc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = tnn.Trainer(\n",
    "    model,\n",
    "    optim,\n",
    "    loss_fn,\n",
    "    trainloader,\n",
    "    testloader,\n",
    "    path=\"./mini-batch.h5\",\n",
    "    device=device,\n",
    "    verbose=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e32f905c-b486-4852-83ba-198c3c2d2703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model using cuda\n",
      "weights saved to ./mini-batch.h5/trajectory/weights-epoch-0\n",
      "training started\n",
      "(epoch: 10): (train loss: 0.1344, test loss: 1.6508, train acc: 0.9577, test acc: 0.6770)\n",
      "weights saved to ./mini-batch.h5/trajectory/weights-epoch-10\n",
      "(epoch: 20): (train loss: 0.0637, test loss: 0.1532, train acc: 0.9794, test acc: 0.9590)\n",
      "weights saved to ./mini-batch.h5/trajectory/weights-epoch-20\n",
      "(epoch: 30): (train loss: 0.0370, test loss: 0.1474, train acc: 0.9870, test acc: 0.9690)\n",
      "weights saved to ./mini-batch.h5/trajectory/weights-epoch-30\n",
      "(epoch: 40): (train loss: 0.0271, test loss: 0.1448, train acc: 0.9905, test acc: 0.9640)\n",
      "weights saved to ./mini-batch.h5/trajectory/weights-epoch-40\n",
      "(epoch: 50): (train loss: 0.0198, test loss: 0.1390, train acc: 0.9932, test acc: 0.9730)\n",
      "weights saved to ./mini-batch.h5/trajectory/weights-epoch-50\n",
      "(epoch: 60): (train loss: 0.0169, test loss: 0.1873, train acc: 0.9942, test acc: 0.9560)\n",
      "weights saved to ./mini-batch.h5/trajectory/weights-epoch-60\n",
      "(epoch: 70): (train loss: 0.0108, test loss: 0.1509, train acc: 0.9964, test acc: 0.9690)\n",
      "weights saved to ./mini-batch.h5/trajectory/weights-epoch-70\n",
      "(epoch: 80): (train loss: 0.0090, test loss: 0.1692, train acc: 0.9975, test acc: 0.9630)\n",
      "weights saved to ./mini-batch.h5/trajectory/weights-epoch-80\n",
      "(epoch: 90): (train loss: 0.0105, test loss: 0.1752, train acc: 0.9965, test acc: 0.9710)\n",
      "weights saved to ./mini-batch.h5/trajectory/weights-epoch-90\n",
      "(epoch: 100): (train loss: 0.0068, test loss: 0.1649, train acc: 0.9979, test acc: 0.9660)\n",
      "weights saved to ./mini-batch.h5/trajectory/weights-epoch-100\n",
      "training complete\n",
      "train_losses saved to ./mini-batch.h5/metrics/train_losses\n",
      "test_losses saved to ./mini-batch.h5/metrics/test_losses\n",
      "train_accs saved to ./mini-batch.h5/metrics/train_accs\n",
      "test_accs saved to ./mini-batch.h5/metrics/test_accs\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.train(epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b8dc8c-d3d5-4a37-8a61-897a24bcdab4",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee414630-64a8-4c73-8b8f-08bfd911636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([ex[\"input\"] for ex in train_dataset]).float().to(device)\n",
    "labels = torch.tensor([ex[\"label\"] for ex in train_dataset]).long().to(device)\n",
    "train_tensor_dataset = data.TensorDataset(inputs, labels)\n",
    "\n",
    "trainloader = data.DataLoader(\n",
    "    train_tensor_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    collate_fn=None,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd0f97d6-14d1-4eb7-b5a0-d2cc10bf8e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tnn.Model(MLP())\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8462c21b-e1b3-418a-bd72-31de88f65b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = tnn.Trainer(\n",
    "    model,\n",
    "    optim,\n",
    "    loss_fn,\n",
    "    trainloader,\n",
    "    testloader,\n",
    "    path=\"./sgd.h5\",\n",
    "    device=device,\n",
    "    verbose=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd46f0f7-ed7b-4431-9d6e-082fe64b53b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model using cuda\n",
      "weights saved to ./sgd.h5/trajectory/weights-epoch-0\n",
      "training started\n",
      "(epoch: 10): (train loss: 2.3026, test loss: 2.3026, train acc: 0.1015, test acc: 0.0950)\n",
      "weights saved to ./sgd.h5/trajectory/weights-epoch-10\n",
      "(epoch: 20): (train loss: 2.3026, test loss: 2.3026, train acc: 0.1015, test acc: 0.0950)\n",
      "weights saved to ./sgd.h5/trajectory/weights-epoch-20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/teaching-neural-networks/tnn/trainer.py:78\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m     75\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 78\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     79\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(logits, labels)\n\u001b[1;32m     80\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/git/teaching-neural-networks/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/teaching-neural-networks/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/git/teaching-neural-networks/tnn/model.py:16\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m     19\u001b[0m     )\n",
      "File \u001b[0;32m~/git/teaching-neural-networks/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/teaching-neural-networks/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 17\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_1(f\u001b[38;5;241m.\u001b[39mrelu(x))\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_2(x))\n",
      "File \u001b[0;32m~/git/teaching-neural-networks/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/teaching-neural-networks/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/git/teaching-neural-networks/venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "metrics = trainer.train(epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
